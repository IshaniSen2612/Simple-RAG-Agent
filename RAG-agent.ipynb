{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8523a96d",
   "metadata": {},
   "source": [
    "## RAG-based Chat Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142dd27d",
   "metadata": {},
   "source": [
    "### Installing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcdf5097",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -q \"langchain[google-genai]\" langchain-google-genai langchain-core langgraph langchain-community gradio ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350e55f3",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910cf942",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "import gradio as gr\n",
    "import requests\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from langgraph.graph import MessagesState, StateGraph, END\n",
    "from langgraph.checkpoint.memory import MemorySaver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98fe056b",
   "metadata": {},
   "source": [
    "### Initialising models and vector store with API Key \n",
    "Get Gemini API key [here](https://aistudio.google.com/apikey)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a5489b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.environ.get(\"GOOGLE_API_KEY\"):\n",
    "    os.environ[\"GOOGLE_API_KEY\"]=getpass.getpass(\"Enter API key for Google Gemini:\")\n",
    "\n",
    "llm=init_chat_model(\"gemini-2.5-flash\", model_provider=\"google_genai\")\n",
    "\n",
    "embeddings= GoogleGenerativeAIEmbeddings(model=\"models/gemini-embedding-001\")\n",
    "\n",
    "vector_store= InMemoryVectorStore(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f101684",
   "metadata": {},
   "source": [
    "### Extracting content from webpage as markdown\n",
    "Here are some example links you can use: \n",
    "- https://lilianweng.github.io/posts/2023-06-23-agent/\n",
    "- https://www.coursera.org/in/articles/what-is-generative-ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffedeaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "webpath=input(\"Enter the url of the webpage to be used as knowledge base:\")\n",
    "web_response=requests.get(f\"https://r.jina.ai/{webpath}\")\n",
    "webcontent=web_response.content\n",
    "webcontent=webcontent.decode()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7aebf2d",
   "metadata": {},
   "source": [
    "### Splitting the web content into chunks and indexing them in a vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091e4ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter=RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "content_chunks= text_splitter.split_text(webcontent)\n",
    "\n",
    "_=vector_store.add_texts(texts=content_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3ef2bc",
   "metadata": {},
   "source": [
    "### Defining the retrieval tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6785d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool(response_format=\"content_and_artifact\")\n",
    "def retrieve(query:str):\n",
    "    \"\"\"Retrieve information related to a query\"\"\"\n",
    "    retrieved_docs= vector_store.similarity_search(query, k=3)\n",
    "    serialized= \"\\n\\n\".join((f\"{doc}\")\n",
    "    for doc in retrieved_docs\n",
    "    )\n",
    "    print (serialized)\n",
    "    return serialized, retrieved_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a00009",
   "metadata": {},
   "source": [
    "### Binding the tool with LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c0ffbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_or_respond(state: MessagesState):\n",
    "    \"\"\"Generate tool call for retrieval or respond\"\"\"\n",
    "    llm_with_tools=llm.bind_tools([retrieve])\n",
    "    response=llm_with_tools.invoke(state[\"messages\"])\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "tools=ToolNode([retrieve])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c702d6",
   "metadata": {},
   "source": [
    "### Defining the function for generating the final response by prompting it with a system message and the retrieved context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884c57ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(state: MessagesState):\n",
    "    \"\"\"Generate answer\"\"\"\n",
    "    recent_tool_messages=[]\n",
    "    for message in reversed(state[\"messages\"]):\n",
    "        if message.type==\"tool\":\n",
    "            recent_tool_messages.append(message)\n",
    "        else:\n",
    "            break\n",
    "    tool_messages=recent_tool_messages[::-1]\n",
    "\n",
    "    #Format into prompt\n",
    "    tool_messages_content=\"\\n\\n\".join(message.content for message in tool_messages)\n",
    "    system_message_content=(\n",
    "        \"You are a helpful assistant who has retrieved the following pieces of information to form a response to the user's query.\"\n",
    "        \"Do not hallucinate.\"\n",
    "        \"If you don't know the answer, say that you don't know.\"\n",
    "        \"\\n\\n\"\n",
    "        f\"{tool_messages_content}\"\n",
    "    )\n",
    "\n",
    "    conversation_messages= [\n",
    "        message\n",
    "        for message in state[\"messages\"]\n",
    "        if message.type in (\"human\", \"system\") or (message.type == \"ai\" and not message.tool_calls)\n",
    "    ]\n",
    "\n",
    "    prompt=[HumanMessage(system_message_content)]+ conversation_messages\n",
    "\n",
    "    final_response=llm.invoke(prompt)\n",
    "    return {\"messages\": [final_response]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9353ec2b",
   "metadata": {},
   "source": [
    "### Building the State Graph of the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8829614b",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_builder=StateGraph(MessagesState)\n",
    "\n",
    "graph_builder.add_node(query_or_respond)\n",
    "graph_builder.add_node(tools)\n",
    "graph_builder.add_node(generate)\n",
    "\n",
    "graph_builder.set_entry_point(\"query_or_respond\")\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"query_or_respond\",\n",
    "    tools_condition,\n",
    "    {END: END, \"tools\": \"tools\"}\n",
    ")\n",
    "\n",
    "graph_builder.add_edge(\"tools\",\"generate\")\n",
    "graph_builder.add_edge(\"generate\", END)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a90a73",
   "metadata": {},
   "source": [
    "### Compiling the graph with Memory Saver to persist chat history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26b4e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory= MemorySaver()\n",
    "graph=graph_builder.compile(checkpointer=memory)\n",
    "\n",
    "#Specifying ID for thread\n",
    "config={\"configurable\":{\"thread_id\":\"abc123\"}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0ef963",
   "metadata": {},
   "source": [
    "### Defining the function to invoke the Agent with the user message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe96f408",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(message, history):\n",
    "    input_data = {\"messages\": [HumanMessage(content=message)]}\n",
    "    agent_response= graph.invoke(input_data,config=config,stream_mode='values')\n",
    "    final_response=agent_response[\"messages\"][-1]\n",
    "    return final_response.content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1435d5",
   "metadata": {},
   "source": [
    "### Starting the Chat UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb220102",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_ui=gr.ChatInterface(\n",
    "    get_response,\n",
    "    type=\"messages\"\n",
    ")\n",
    "\n",
    "chat_ui.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Agent-using-RAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
